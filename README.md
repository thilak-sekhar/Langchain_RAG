# Enterprise Knowledge RAG Assistant

A **production-style Retrieval-Augmented Generation (RAG) system** built using **LangChain, Pinecone, and Streamlit**.  
The system answers user queries **strictly grounded in ingested documents**, demonstrating clean architecture, incremental ingestion, and real-world usability.

This project is intentionally designed as a **maintainable AI system**, not a chatbot demo.

---

## ğŸ” Key Features

- Incremental PDF ingestion (no Pinecone index reset)
- Vector-based semantic search using Pinecone
- Retrieval-Augmented Generation (RAG) with LangChain
- Clean system entrypoint reusable across CLI and UI
- Streamlit-based interactive UI
- Transparent source attribution
- Modular, production-aligned project structure

---

## ğŸ§  Architecture Overview

```

Streamlit UI
â†“
System Entrypoint (main.py)
â†“
RAG Chain (LangChain)
â†“
Retriever + Prompt + LLM
â†“
Pinecone Vector Store

```

**Design Principles**
- UI is a thin client (no business logic)
- All orchestration flows through `main.py`
- RAG logic is isolated and reusable
- Data ingestion is incremental and safe

---

## ğŸ“‚ Project Structure

```

enterprise-rag-assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ chains/          # RAG chain logic
â”‚   â”œâ”€â”€ config/          # Configurations
â”‚   â”œâ”€â”€ embeddings/      # Embedding logic
â”‚   â”œâ”€â”€ ingestion/       # PDF loading, chunking, ingestion
â”‚   â”œâ”€â”€ retriever/       # Pinecone retriever
â”‚   â”œâ”€â”€ ui/              # Streamlit UI
â”‚   â”œâ”€â”€ utils/           # Logger and helpers
â”‚   â””â”€â”€ main.py          # System entrypoint
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdf/
â”‚       â””â”€â”€ langchain_deep_dive.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```

---

## ğŸ“˜ Demo Data

The repository includes a **5+ page LangChain deep-dive PDF** used to demonstrate:

- Chunking quality
- Cross-section retrieval
- Concept-level grounding
- Hallucination resistance

File:
```

data/pdf/langchain_deep_dive.pdf

````

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
````

---

### 2ï¸âƒ£ Ingest Documents

```bash
python app/ingestion/ingest_pipeline.py
```

This **appends documents** to the existing Pinecone index without resetting it.

---

### 3ï¸âƒ£ Run Streamlit UI

```bash
streamlit run app/ui/streamlit_app.py
```

---

### 4ï¸âƒ£ (Optional) CLI Mode

```bash
python app/main.py
```

---

## ğŸ§ª Example Demo Queries

Use these to validate RAG behavior:

* What is LangChain and why is it used in production systems?
* Explain how LangChain supports Retrieval-Augmented Generation.
* What are agents in LangChain?
* What are best practices for using LangChain in production?

Out-of-scope test:

* What is quantum teleportation over blockchain?

(Expected: refusal or low-confidence response)

---

## ğŸ›  Key Design Decisions

* **Precision-first retrieval** to reduce hallucinations
* Incremental ingestion for production safety
* UI kept separate from orchestration logic
* No direct LLM calls from UI
* No hardcoded prompt logic in frontend

---

## ğŸ“Œ Use Cases

* Enterprise document Q&A
* Internal knowledge assistants
* Policy and technical documentation exploration
* Analyst and developer support tools

---

## ğŸ“ˆ Future Enhancements

* Offline evaluation & regression tests
* FastAPI service layer
* Dockerized deployment
* CI-based guardrails
* Document versioning and metadata filtering

---

## ğŸ‘¤ Author

Built as a **production-grade RAG system** with emphasis on:
**architecture, correctness, and maintainability** â€” not shortcuts.

```
